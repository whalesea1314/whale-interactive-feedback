//! MCP (Model Context Protocol) æœåŠ¡å™¨æ¨¡å—

use std::sync::Arc;
use rmcp::{
    ServerHandler, ServiceExt,
    handler::server::{router::tool::ToolRouter, wrapper::Parameters},
    model::{ServerCapabilities, ServerInfo, ListToolsResult, PaginatedRequestParam, Tool},
    service::RequestContext,
    schemars, tool, tool_router, RoleServer, ErrorData as McpError,
};
use serde::{Deserialize, Serialize};

use crate::popup::{PopupRequest, launch_popup_and_wait, cleanup_request_file};

/// MCP å·¥å…·è°ƒç”¨å‚æ•° - interactive_feedback
#[derive(Debug, Clone, Serialize, Deserialize, schemars::JsonSchema)]
pub struct InteractiveFeedbackParams {
    #[schemars(description = "Summary of the changes or work done by the AI that needs user review")]
    pub message: String,
    
    #[serde(default)]
    #[schemars(description = "Full detailed content (optional, shown in expandable section)")]
    pub full_response: Option<String>,
    
    #[serde(default)]
    #[schemars(description = "List of predefined options for the user to choose from")]
    pub predefined_options: Option<Vec<String>>,
}

/// MCP å·¥å…·è°ƒç”¨å‚æ•° - optimize_user_input
#[derive(Debug, Clone, Serialize, Deserialize, schemars::JsonSchema)]
pub struct OptimizeUserInputParams {
    #[schemars(description = "éœ€è¦ä¼˜åŒ–çš„ç”¨æˆ·è¾“å…¥æ–‡æœ¬")]
    pub text: String,
    
    #[schemars(description = "ä¼˜åŒ–æ¨¡å¼: 'optimize' è¿›è¡Œæ ‡å‡†ä¼˜åŒ–, 'enhance' ä½¿ç”¨è‡ªå®šä¹‰å¢å¼ºæŒ‡ä»¤")]
    pub mode: Option<String>,
    
    #[schemars(description = "è‡ªå®šä¹‰å¢å¼ºæŒ‡ä»¤ï¼Œä»…åœ¨ mode ä¸º 'enhance' æ—¶ä½¿ç”¨")]
    pub custom_prompt: Option<String>,
}

/// ä¼˜åŒ–ç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OptimizeResult {
    pub optimized_text: String,
    pub success: bool,
    pub error: Option<String>,
}

/// MCP æœåŠ¡å™¨
#[derive(Debug, Clone)]
pub struct McpServer {
    tool_router: ToolRouter<Self>,
}

#[tool_router]
impl McpServer {
    pub fn new() -> Self {
        Self {
            tool_router: Self::tool_router(),
        }
    }

    /// whale_interactive_feedback å·¥å…· - å¯åŠ¨ GUI å¼¹çª—æ”¶é›†ç”¨æˆ·åé¦ˆ
    #[tool(
        name = "whale_interactive_feedback",
        description = "Request interactive feedback from the user. Opens a popup for the user to review AI's work and provide feedback, select options, or attach images."
    )]
    async fn interactive_feedback(
        &self,
        Parameters(params): Parameters<InteractiveFeedbackParams>,
    ) -> String {
        log::info!("interactive_feedback called with message: {}", params.message);
        
        // åˆ›å»º popup è¯·æ±‚
        let request = PopupRequest::new(
            Some(params.message.clone()),
            params.full_response.clone(),
            params.predefined_options.clone(),
        );
        let request_id = request.id.clone();
        
        // å¯åŠ¨ GUI å¹¶ç­‰å¾…å“åº”
        match launch_popup_and_wait(&request).await {
            Ok(response) => {
                // æ¸…ç†è¯·æ±‚æ–‡ä»¶
                if let Err(e) = cleanup_request_file(&request_id).await {
                    log::warn!("Failed to cleanup request file: {}", e);
                }
                
                if response.cancelled {
                    return "[User cancelled or provided no feedback]".to_string();
                }
                
                // æ ¼å¼åŒ–ç»“æœ
                let mut parts = Vec::new();
                
                if !response.selected_options.is_empty() {
                    parts.push(format!("**Selected Options:** {}", response.selected_options.join(", ")));
                }
                
                if let Some(ref feedback) = response.user_input {
                    if !feedback.is_empty() {
                        parts.push(format!("**User Feedback:**\n{}", feedback));
                    }
                }
                
                if !response.images.is_empty() {
                    parts.push(format!("**Attached Images:** {} image(s)", response.images.len()));
                }
                
                if !response.file_references.is_empty() {
                    let file_list: Vec<String> = response.file_references.iter()
                        .map(|f| {
                            let icon = if f.is_directory { "ğŸ“" } else { "ğŸ“„" };
                            format!("{} {}", icon, f.path)
                        })
                        .collect();
                    parts.push(format!("**Attached Files:**\n{}", file_list.join("\n")));
                }
                
                if parts.is_empty() {
                    "No feedback provided by user.".to_string()
                } else {
                    parts.join("\n\n")
                }
            }
            Err(e) => {
                let _ = cleanup_request_file(&request_id).await;
                log::error!("Failed to get feedback: {}", e);
                format!("Error: Failed to get user feedback - {}", e)
            }
        }
    }

    /// whale_optimize_user_input å·¥å…·
    #[tool(
        name = "whale_optimize_user_input",
        description = "ä½¿ç”¨ AI ä¼˜åŒ–ç”¨æˆ·è¾“å…¥æ–‡æœ¬ï¼Œå°†å£è¯­åŒ–è¾“å…¥è½¬æ¢ä¸ºç»“æ„åŒ–æŒ‡ä»¤ã€‚"
    )]
    async fn optimize_user_input(
        &self,
        Parameters(params): Parameters<OptimizeUserInputParams>,
    ) -> String {
        if params.text.trim().is_empty() {
            return "Error: 'text' å‚æ•°ä¸èƒ½ä¸ºç©º".to_string();
        }
        
        let mode = params.mode.as_deref().unwrap_or("optimize");
        
        if mode != "optimize" && mode != "enhance" {
            return "Error: 'mode' å‚æ•°å¿…é¡»æ˜¯ 'optimize' æˆ– 'enhance'".to_string();
        }
        
        if mode == "enhance" && params.custom_prompt.is_none() {
            return "Error: å½“ mode ä¸º 'enhance' æ—¶ï¼Œå¿…é¡»æä¾› 'custom_prompt' å‚æ•°".to_string();
        }
        
        log::info!("optimize_user_input å·¥å…·è¢«è°ƒç”¨ï¼Œæ¨¡å¼: {}", mode);
        
        // ç›´æ¥ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®
        let config = match crate::config::load_config_direct().await {
            Ok(c) => c,
            Err(e) => return format!("Error: åŠ è½½é…ç½®å¤±è´¥: {}", e),
        };
        
        // è·å–å·²é…ç½®çš„æä¾›å•†å’Œ API å¯†é’¥
        let (provider_name, obfuscated_key) = if let Some(ref key) = config.api_keys.openai {
            if !key.is_empty() { ("openai", key.clone()) } else { ("", String::new()) }
        } else if let Some(ref key) = config.api_keys.gemini {
            if !key.is_empty() { ("gemini", key.clone()) } else { ("", String::new()) }
        } else if let Some(ref key) = config.api_keys.deepseek {
            if !key.is_empty() { ("deepseek", key.clone()) } else { ("", String::new()) }
        } else if let Some(ref key) = config.api_keys.volcengine {
            if !key.is_empty() { ("volcengine", key.clone()) } else { ("", String::new()) }
        } else {
            return "Error: æœªé…ç½®ä»»ä½• API å¯†é’¥ï¼Œè¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½®".to_string();
        };
        
        if provider_name.is_empty() {
            return "Error: æœªé…ç½®ä»»ä½• API å¯†é’¥ï¼Œè¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½®".to_string();
        }
        
        // è§£æ··æ·† API å¯†é’¥
        let api_key = match crate::api_keys::ApiKeyManager::deobfuscate(&obfuscated_key) {
            Ok(key) => key,
            Err(e) => return format!("Error: è§£å¯† API å¯†é’¥å¤±è´¥: {}", e),
        };
        
        // åˆ›å»º LLM é…ç½®
        let config = match crate::llm::LlmConfig::from_provider(provider_name, api_key) {
            Some(c) => c,
            None => return format!("Error: ä¸æ”¯æŒçš„æä¾›å•†: {}", provider_name),
        };
        
        // åˆ›å»º Provider
        let llm = match crate::llm::LlmProvider::new(config) {
            Ok(l) => l,
            Err(e) => return format!("Error: åˆ›å»º LLM Provider å¤±è´¥: {}", e),
        };
        
        // è·å–ä¼˜åŒ–ç±»å‹
        let opt_type = if mode == "enhance" {
            crate::llm::OptimizationType::Reinforce
        } else {
            crate::llm::OptimizationType::Optimize
        };
        
        // è·å–æç¤ºè¯
        let system_prompt = crate::llm::get_optimization_prompt(opt_type, params.custom_prompt.as_deref());
        
        // è°ƒç”¨ LLM
        match llm.optimize_text(&params.text, &system_prompt).await {
            Ok(result) => result,
            Err(e) => format!("Error: ä¼˜åŒ–å¤±è´¥: {}", e),
        }
    }
}

impl Default for McpServer {
    fn default() -> Self {
        Self::new()
    }
}

/// ç§»é™¤ JSON Schema ä¸­çš„ $schema å­—æ®µï¼Œè§£å†³ Kiro ä¸æ”¯æŒ draft/2020-12 çš„é—®é¢˜
fn remove_schema_field(tool: &Tool) -> Tool {
    let mut new_schema = tool.input_schema.as_ref().clone();
    new_schema.remove("$schema");
    
    Tool {
        name: tool.name.clone(),
        title: tool.title.clone(),
        description: tool.description.clone(),
        input_schema: Arc::new(new_schema),
        output_schema: tool.output_schema.clone(),
        annotations: tool.annotations.clone(),
        icons: tool.icons.clone(),
        meta: tool.meta.clone(),
    }
}

impl ServerHandler for McpServer {
    fn get_info(&self) -> ServerInfo {
        ServerInfo {
            instructions: Some(
                "Whale Interactive Feedback MCP æœåŠ¡å™¨ - é€šè¿‡ GUI å¼¹çª—æ”¶é›†ç”¨æˆ·åé¦ˆ".into()
            ),
            capabilities: ServerCapabilities::builder().enable_tools().build(),
            ..Default::default()
        }
    }
    
    fn list_tools(
        &self,
        _request: Option<PaginatedRequestParam>,
        _context: RequestContext<RoleServer>,
    ) -> impl std::future::Future<Output = Result<ListToolsResult, McpError>> + Send + '_ {
        async move {
            // è·å–åŸå§‹å·¥å…·åˆ—è¡¨
            let tools = self.tool_router.list_all();
            
            // ç§»é™¤æ¯ä¸ªå·¥å…· schema ä¸­çš„ $schema å­—æ®µ
            let fixed_tools: Vec<Tool> = tools.iter().map(remove_schema_field).collect();
            
            Ok(ListToolsResult {
                tools: fixed_tools,
                next_cursor: None,
                meta: Default::default(),
            })
        }
    }
    
    fn call_tool(
        &self,
        request: rmcp::model::CallToolRequestParam,
        context: RequestContext<RoleServer>,
    ) -> impl std::future::Future<Output = Result<rmcp::model::CallToolResult, McpError>> + Send + '_ {
        use rmcp::handler::server::tool::ToolCallContext;
        let tool_context = ToolCallContext::new(self, request, context);
        self.tool_router.call(tool_context)
    }
}

/// è¿è¡Œ MCP æœåŠ¡å™¨
pub async fn run_mcp_server() -> anyhow::Result<()> {
    log::info!("å¯åŠ¨ MCP æœåŠ¡å™¨...");
    
    let server = McpServer::new();
    let transport = rmcp::transport::io::stdio();
    let server_handle = server.serve(transport).await?;
    
    log::info!("MCP æœåŠ¡å™¨å·²å¯åŠ¨ï¼Œç­‰å¾…è¿æ¥...");
    
    server_handle.waiting().await?;
    
    log::info!("MCP æœåŠ¡å™¨å·²å…³é—­");
    Ok(())
}

// ä¿ç•™æ—§çš„å¯¼å‡ºä»¥å…¼å®¹
pub use crate::popup::PopupResponse;

/// éªŒè¯ interactive_feedback å‚æ•°
pub fn validate_interactive_feedback_params(params: &InteractiveFeedbackParams) -> Result<(), String> {
    if params.message.trim().is_empty() {
        return Err("'message' å‚æ•°ä¸èƒ½ä¸ºç©º".to_string());
    }
    
    if let Some(ref options) = params.predefined_options {
        if options.iter().any(|opt| opt.trim().is_empty()) {
            return Err("predefined_options ä¸­ä¸èƒ½åŒ…å«ç©ºå­—ç¬¦ä¸²".to_string());
        }
    }
    
    Ok(())
}

/// éªŒè¯ optimize_user_input å‚æ•°
pub fn validate_optimize_user_input_params(params: &OptimizeUserInputParams) -> Result<(), String> {
    if params.text.trim().is_empty() {
        return Err("'text' å‚æ•°ä¸èƒ½ä¸ºç©º".to_string());
    }
    
    if let Some(ref mode) = params.mode {
        if mode != "optimize" && mode != "enhance" {
            return Err("'mode' å‚æ•°å¿…é¡»æ˜¯ 'optimize' æˆ– 'enhance'".to_string());
        }
        
        if mode == "enhance" && params.custom_prompt.is_none() {
            return Err("å½“ mode ä¸º 'enhance' æ—¶ï¼Œå¿…é¡»æä¾› 'custom_prompt' å‚æ•°".to_string());
        }
    }
    
    Ok(())
}
